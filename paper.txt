Our method consists of three main stages. First, an input image is passed through a CNN-based feature extractor (ResNet-50) to obtain multi-scale feature maps. Second, the extracted features are fed into a Transformer encoder-decoder module, where self-attention and cross-attention mechanisms capture long-range dependencies. Finally, a lightweight MLP head produces the final classification output. Skip connections link the feature extractor to the decoder to preserve spatial information.